{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORilW357y+bS8ULNGVTI/w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prhvmm/Gradient-Descent-from-scratch-on-Boston-Dataset/blob/master/Gradient_Descent_from_scratch_on_Boston_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "boston = load_boston()\n",
        "\n",
        "\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "scaler = preprocessing.StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "x0 = np.ones((np.size(X, 0), 1))\n",
        "X = np.hstack((x0, X))\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, shuffle = False)\n",
        "\n",
        "m = np.size(X_train, 0) \n",
        "\n",
        "theta = np.ones((np.size(X_train, 1)))\n",
        "\n",
        "error = np.zeros(m)\n",
        "\n",
        "alpha = 0.0001\n",
        "\n",
        "lamb = 100\n",
        "\n",
        "cost = []\n",
        "\n",
        "threshold = 0.0001\n",
        "\n",
        "\n",
        "def mean_squared_error_regularization(theta):\n",
        "    for i in range(m):\n",
        "        error[i] = (((theta.transpose()) @ X_train[i,:]) - (Y_train[i]))   \n",
        "    regularized = sum(np.power(theta, 2))\n",
        "    MSE = 1/2 * (sum(np.power(error, 2)) + (lamb * regularized)) / m\n",
        "    return MSE\n",
        "\n",
        "def mean_absolute_error(theta):\n",
        "    for i in range(m):\n",
        "        error[i] = ((theta.transpose()) @ X_train[i,:]) - (Y_train[i])\n",
        "    MAE = np.mean(abs(error))\n",
        "    return MAE\n",
        "\n",
        "def mean_squared_error(theta):\n",
        "    for i in range(m):\n",
        "        error[i] = ((theta.transpose()) @ X_train[i,:]) - (Y_train[i])\n",
        "    MSE = 1/2 * np.mean(np.power(error, 2))\n",
        "    return MSE\n",
        "\n",
        "\n",
        "def gradient_descent(theta):\n",
        "    cost.append(mean_squared_error(theta)) #you can change to mean_absolute_error\n",
        "    new_theta = np.zeros((np.size(X_train, 1)))\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        for j in range(np.size(theta, 0)):\n",
        "            temp = np.multiply(error,X_train[:, j])\n",
        "            new_theta[j] = theta[j] - ((alpha) * np.mean(temp))\n",
        "        theta = new_theta\n",
        "        cost.append(mean_squared_error(theta))\n",
        "        iteration += 1\n",
        "        if cost[-2] - cost[-1] < threshold:\n",
        "            break\n",
        "\n",
        "\n",
        "    print('Best Cost:' + str(cost[-1]))\n",
        "    print('Iterations:' + str(iteration))\n",
        "    plt.plot(cost)\n",
        "    plt.title(\"alpha = %f\" %alpha)\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Cost\")\n",
        "    plt.show()\n",
        "\n",
        "    final = np.zeros((np.size(X_test,0)))\n",
        "    for i in range(np.size(X_test,0)):\n",
        "        final[i] = ((theta.transpose()) @ X_test[i,:]) - (Y_test[i])\n",
        "    final_MSE = 1/2 * np.mean(np.power(final,2))\n",
        "    print('Accuracy in the test data is: %f' %final_MSE)\n",
        "    print(theta)\n",
        "\n",
        "def gradient_descent_regularization(theta):\n",
        "    cost.append(mean_squared_error_regularization(theta))\n",
        "    new_theta = np.zeros((np.size(X_train, 1)))\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        for i in range(np.size(theta, 0)):\n",
        "            temp = np.multiply(error,X_train[:, i])\n",
        "            if(i==0):\n",
        "                new_theta[i] = theta[i] - ((alpha) * np.mean(temp))\n",
        "            else:\n",
        "                new_theta[i] = theta[i] * (1 - ((alpha/m) * lamb)) - ((alpha) * np.mean(temp))\n",
        "        theta = new_theta\n",
        "        cost.append(mean_squared_error_regularization(theta))\n",
        "        iteration += 1\n",
        "        if cost[-2] - cost[-1] < threshold:\n",
        "            break\n",
        "    \n",
        "    print('Best Cost:' + str(cost[-1]))\n",
        "    print('Iterations:' + str(iteration))\n",
        "    plt.plot(cost)\n",
        "    plt.title(\"lambda = %i\" %lamb)\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Cost\")\n",
        "    plt.show()\n",
        "\n",
        "    final = np.zeros((np.size(X_test,0)))\n",
        "    for i in range(np.size(X_test,0)):\n",
        "        final[i] = ((theta.transpose()) @ X_test[i,:]) - (Y_test[i])\n",
        "        regularized = sum(np.power(theta, 2))\n",
        "    final_MSE = 1/2 * (sum(np.power(final, 2)) + (lamb * regularized))/(np.size(X_test,0))\n",
        "    print('Accuracy in the test data is: %f' %final_MSE)\n",
        "    print(theta)\n",
        "\n",
        "#gradient_descent(theta) #without regularization\n",
        "gradient_descent_regularization(theta) #with regularization\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gM5d8r-3HWv2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}